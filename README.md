# 使用R语言爬取文本并训练文本分类模型


这是我的一项课程作业，做得还比较粗糙，欢迎大家批评指正！


## 整体的训练思路如下
  
- 研究背景：情感分析
- 数据获取：爬取豆瓣短评
- 数据预处理：文本清洗、中文分词、停用词过滤、词频分析
- 特征工程：词频矩阵与TF-IDF矩阵
- 建立模型：神经网络模型
- 优化模型：最优网络结构

## R语言爬虫
  大一上python课程的时候，个人的好奇心驱使尝试用python爬取了数据，但这次作业老师要求用R语言。

  
  网络上关于R语言的爬虫教程不多，而且没有一个很权威的。一开始检索资料花了很长时间。
  
  
  大部分教程都是使用rvest包，但是我不管怎么调整，都会被豆瓣反爬。
  
  
  于是我改成用Rselenium来爬取，参考up主笔记https://www.bilibili.com/opus/653824890129874965?spm_id_from=333.1007.0.0进行环境配置。

  
  需要安装java，之后每次运行需要在cmd输入这行代码：
<img width="1475" height="379" alt="image" src="https://github.com/user-attachments/assets/ab4e32ba-f458-4ed8-bf51-060faa8410ff" />


- 定义一个抓取评论函数scrape_comments(url, output_file)，循环提取评论直到500条或无法获取更多。获取页面源代码并读取成html代码，从html中找到comment-item下short类，读取成文本，添加到总评论集。由于一页有显示数量限制，找到next元素并点击，如果无法点击就退出循环。保存结果到数据框和csv文件。

## 数据清洗与特征工程

- 保留中文字符和换行符，去除所有其他字符（如英文、数字、标点等），并去掉空行和空评论。用了一个匿名函数function（x）来遍历评论
- 使用Rwordseg进行分词（一开始想用jieba包，但是这个包安装不好，改天重新试试），过滤停用词表（这个词表是我从github随便扒的，我觉得这样并不严谨，后续再优化一下）
- 过滤前后进行词频统计以及高频词排序，可以根据这个排序来修改停用词表，去除一些无效信息（但是要怎么定义无效信息呢？这是一个问题）。然后构建了文本稀疏矩阵TF-IDF（学大模型的时候发现别人好像有其他的构建方法）

  
  ## 模型构建与优化
  
- 把数据类别转换成0或1，划分数据成训练集和测试集（7：3）。这里直接选择了前100个（不然特征太多啦！）最具区分性（指标：正面TF-IDF和负面TF-IDF之间的差值越大就越有区分性）的特征，用来构建用于神经网络的数据集（也就是说有100个变量~），并进行数据标准化。然后用neuralnet构建神经网络模型。测试并评估性能（准确率、灵敏度、特异性、AUC值）。
- 从模型隐藏层配置入手来优化模型，定义多种隐藏层配置，分别测试并在测试集上进行预测。按准确率排序得到最佳配置，重新训练模型，然后把优化结果可视化（柱状图、ROC曲线）。

